# -*- coding: utf-8 -*-
"""TP_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1th-Wrogmkgl7S_pipFptcTN3WHNKCDRv
"""

!pip install --upgrade geopandas

!pip install --upgrade ptitprince

!pip install --upgrade joypy

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import ptitprince as pt
import sys
import geopandas
import seaborn as sns
import joypy
import matplotlib.pyplot as plt
from plotly import graph_objects as go
from wordcloud import WordCloud, STOPWORDS
from matplotlib.colors import ListedColormap
from matplotlib import cm
# %matplotlib inline

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# Authenticate and create the PyDrive client
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

id='1Pk5MK9Hs_kMUT9NotGnOKE0NPra-39YU'
downloaded = drive.CreateFile({'id': id})
downloaded.GetContentFile('train.csv')

train = pd.read_csv('train.csv', encoding='latin-1')
train['longitud'] = train['text'].apply(len)
train.head(500)

from google.colab import drive
drive.mount('/content/drive')

train.count()

train.info()

"""#Datos relevantes

Porcentaje de tweets falsos vs verídicos
"""

verdaderos = train['target'].sum()
falsos = train['target'].count() - train['target'].sum()
verdaderos

true_vs_false = pd.DataFrame({'cantidad tweets': [train['target'].sum(), train['target'].count() - train['target'].sum()], 'porcentaje':[train['target']]}, index=['verdaderos', 'falsos'])
true_vs_false.plot.bar(rot=0, title='Cantidad total de tweets verdaderos y falsos', color='pink')

tw_mas_largo = train['longitud'].max()
tw_mas_largo

train.max()

train.min()

"""#Filtrado de datos por localidad

Filtrado manual del DataFrame las localidades que no existen o son vacias/NaN.
"""

df_filter_location = train.copy()
del df_filter_location['id']
df_filter_location.dropna(subset=['location'], inplace = True)
df_filter_location = df_filter_location[df_filter_location.location != '  ']
df_filter_location.count()

def eliminar_localidades_no_existentes(location):
  if (location.find('www.') != -1 or location.find('WWW.') != -1):
    return np.nan
  if (location.find(',') != -1 or location.find('.') != -1):
    loc = location.replace(',', '')
    loc = location.replace('.', '')
    if loc.isalpha() and (len(loc) < 100): #valuación del estado del csv a analizar
     return location.upper()
  if location.isalpha() and (len(location) < 100):
    return location.upper()
  return np.nan

df_filter_location['location'] = df_filter_location['location'].transform(eliminar_localidades_no_existentes)
df_filter_location.dropna(subset=['location'], inplace = True)
df_filter_location.count()

df_filter_location['country'] = df_filter_location['location']
df_filter_location.head()

df_filter_location['country'] = df_filter_location['location'].replace("U.S.A", "USA")
df_filter_location['country'] = df_filter_location['location'].replace("UNITED STATES", "USA")
df_filter_location['country'] = df_filter_location['location'].replace("U.S", "USA")
df_filter_location['country'] = df_filter_location['location'].replace("US", "USA")
df_filter_location.country.value_counts()

"""#Filtrado de tweets por keyword

Filtro del DataFrame los tweets cuya keyword sea NaN
"""

df_filter_keyword_sin_nan = train.copy()
df_filter_keyword_sin_nan.dropna(subset=['keyword'], inplace = True)
df_filter_keyword_sin_nan.head(400)

df_keyword_con_nan = train.copy()
df_keyword_con_nan['keyword'] = df_keyword_con_nan[df_keyword_con_nan['keyword'] == np.nan]
df_keyword_con_nan.head()

"""#ANALISIS EXPLORATORIO

¿En qué localidad hay más tweets? USA
"""

cantidad_tweets_location = df_filter_location.loc[:,['text', 'location']].groupby('location').agg({'text': 'count'})
cantidad_de_tweets_por_localidad_ordenado = cantidad_tweets_location.sort_values(by='text', ascending=False).head(10)
cantidad_de_tweets_por_localidad_ordenado

"""Grafico de barras -  Cantidad de tweets por localidad"""

fig, ax = plt.subplots(figsize = (15, 7))
g = sns.barplot(x=cantidad_de_tweets_por_localidad_ordenado['text'], \
                y=cantidad_de_tweets_por_localidad_ordenado.index, orient='h',ax = ax,)
g.set_title("Top 10 localidades con mayor cantidad de tweets", fontsize=15)
g.set_xlabel("cantidades", fontsize=12)
g.set_ylabel("localidades", fontsize=12)
plt.savefig("top 10 localidades.png")

"""Vericidad de los tweets por localidad"""

cantidad_tweets_veridicos_location = df_filter_location.loc[:, ['location', 'target']].groupby('location').agg({'target': ['count','sum']})
cantidad_tweets_veridicos_location.columns = ['Cantidad total tweets', 'Cantidad verídicos']
df = cantidad_tweets_veridicos_location.sort_values(by='Cantidad verídicos', ascending=False).head(10)
df

"""Top 10 localidades - localidades con más tweets"""

df_veracidad_location = cantidad_tweets_veridicos_location.sort_values(by='Cantidad total tweets', ascending=False).head(10)
df_veracidad_location

df_veracidad_location.plot(kind = 'bar',
             width=0.8,
             figsize=(15,7), title='Relacion localidad veracidad')

"""Distribucion de los largos de los tweets en la localidad con mas tweets (USA)"""

localidad_con_mas_tweets = df_filter_location.groupby('location').get_group('USA')
localidad_con_mas_tweets

localidad_con_mas_tweets_veridicos = localidad_con_mas_tweets[localidad_con_mas_tweets['target'] == 1]
localidad_con_mas_tweets_falsos = localidad_con_mas_tweets[localidad_con_mas_tweets['target'] != 1]

x= localidad_con_mas_tweets_veridicos['longitud']
y= localidad_con_mas_tweets_falsos['longitud']
sns.distplot(x, hist=False,color="blue", label= 'tweets veridicos')
sns.distplot(y, hist=False,color="red", label= 'tweets falsos').set_title('Distribucion longitudes de tweets - USA')

"""Comparación porcentajes de verídicos con respecto a falsos en cada localidad"""

def contar_falsos(x):
  return x.count() - x.sum()

def porcentaje_veridicos(x):
  return (x.sum()/x.count())*100

comparacion_tweets_veridicos_location = df_filter_location.groupby('location').agg({'target': ['sum', contar_falsos]})
#comparacion_tweets_veridicos_location.sort_values(by='target', ascending=False).head()
comparacion_tweets_veridicos_location.columns = ['Tweets verdaderos', 'Tweets falsos']
comparacion_tweets_veridicos_location = comparacion_tweets_veridicos_location.sort_values(by='Tweets verdaderos', ascending=False)
comparacion_tweets_veridicos_location

"""Relacion entre la veracidad del tweet y la keyword."""

cantidad_veridicos_sin_nan = df_filter_keyword_sin_nan['target'].sum()
cantidad_falsos_sin_nan = df_filter_keyword_sin_nan['target'].count() - cantidad_veridicos_sin_nan
promedio_de_veridicos_sin_nan = (cantidad_veridicos_sin_nan / df_filter_keyword_sin_nan['keyword'].count() * 100) 
promedio_de_veridicos_sin_nan

hola = df_filter_keyword_sin_nan.groupby('keyword').agg({'text':'count'})
hola.count()

cantidad_veridicos_con_nan = df_keyword_con_nan['target'].sum()
cantidad_falsos_con_nan = df_keyword_con_nan['target'].count() - cantidad_veridicos_con_nan
promedio_de_veridicos_con_nan = (cantidad_veridicos_con_nan / df_keyword_con_nan['target'].count() * 100)
promedio_de_veridicos_con_nan

plt.style.use('ggplot')
df_grafico = pd.DataFrame({'cantidad con keyword': [cantidad_veridicos_sin_nan, cantidad_falsos_sin_nan], 'cantidad sin keyword': [cantidad_veridicos_con_nan, cantidad_falsos_con_nan]}, index=['verídicos', 'falsos'])
df_grafico.plot.bar(rot=0, figsize=(15,7), title='Relacion keyword - veracidad')

"""Cantidad de tweets por keyword. Relación con la veracidad"""

groupedby_keyword = df_filter_keyword_sin_nan.loc[:, ['keyword', 'target']].groupby('keyword').agg({'target': ['count','sum']})
groupedby_keyword.columns = ['Cantidad total tweets', 'Cantidad verídicos']
groupedby_keyword.head()

df = groupedby_keyword.sort_values(by='Cantidad total tweets', ascending=False).head(10)
df.index = df.index.str.replace('%20', ' ')

bar = df.plot(kind = 'bar',width=0.8,figsize=(15,7), title="Top 10 keywords con mas tweets")

"""Búsqueda de localidades de los tweets en las keyword mas utilizadas."""

df_all_filters = df_filter_location.copy()
df_all_filters.dropna(subset=['keyword'], inplace = True)
df_all_filters['keyword'] = df_all_filters['keyword'].str.replace('%20', ' ')

tabla = df_all_filters[['keyword', 'location', 'target']].pivot_table(index='keyword',columns='location',values='target', aggfunc='count')
tabla

tabla.to_csv('tabla.csv')

cant_k_loc = df_all_filters.groupby('location').agg({'keyword':'count'})
cant_k_loc.sort_values(by='keyword', ascending=False)

"""En qué localidad hay más tweets que el promedio?"""

cantidad_t_localidad = df_filter_location.groupby('location').agg({'target':'count'})
promedio = cantidad_t_localidad['target'].sum()/cantidad_t_localidad.count()
localidades_con_mas_tweets_que_promedio = cantidad_t_localidad[cantidad_t_localidad['target']> int(promedio)]
localidades_con_mas_tweets_que_promedio.head()

"""Para gráfico longitud vs frecuencia"""

tweets_por_longitud = df_all_filters.groupby('longitud').agg({'target':'count'})
new_index = ['id']
tweets_por_longitud['id'] = 0
tweets_por_longitud = tweets_por_longitud.unstack()
tweets_por_longitud.columns = ['longitud', 'cantidad']
tweets_por_longitud
df_all_filters

"""Relacion keyword - localidad"""

tabla = df_all_filters[['keyword', 'location', 'target']].pivot_table(index='keyword',columns='location',values='target', aggfunc='count')
maxValuesObj = tabla.idxmax(axis=1)
maxCantidades = tabla.max(axis=1)
d = {'localidades maximas': maxValuesObj, 'cantidad por localidad': maxCantidades}
maximas_localidades_en_keyword = pd.DataFrame(data=d)
maximas_localidades_en_keyword = maximas_localidades_en_keyword.reset_index().rename(columns={df.index.name: 'keyword'})
maximas_localidades_en_keyword

"""Distribución de la longitud de acuerdo a la veracidad"""

fig, ax = plt.subplots(figsize = (9, 9))
g = sns.boxplot(x='target', y='longitud', data=df_all_filters, ax= ax)
g.set_title('Distribución de la longitud según veracidad', fontsize=18);
g.set_ylabel('Longitud');
g.set_xlabel('Target');
g.set_xticklabels(g.get_xticklabels(), rotation=45);

"""Veracidad por longitud de tweets"""

def target_longitud(x):
  if x.sum() > x.count() - x.sum():
    return 1
  return 0

grouped_target_long = df_all_filters.groupby(['longitud']).agg({'target': ['count', target_longitud]})
grouped_target_long.columns = ['Cantidad de tweets', 'Target']
grouped_target_long['Longitud del tweet'] = grouped_target_long.index
grouped_target_long

grouped_target_long



sns.set()
g = sns.relplot(x='Longitud del tweet', y='Cantidad de tweets', col='Target', data=grouped_target_long)
g.fig.suptitle("Distribución cantidad de tweets - longitud", x=0.52, y=1)

"""Distribucion del target en funcion de la longitud"""

ax = pt.RainCloud(x = 'target', y = 'longitud',
                 data = train,
                 width_viol = .8,
                 width_box = .4,
                 figsize = (50, 50),
                 orient = 'h')
ax.set_title('Distribucion del target en funcion de la longitud')

"""#MAPA"""

world = geopandas.read_file(geopandas.datasets.get_path('naturalearth_lowres'))
world.count()

train.columns = ['id', 'keyword', 'continent', 'text', 'target', 'longitud']
world_train_1 = pd.merge(world, train, how='inner', on='continent')
world_train_1.count()

train.columns = ['id', 'keyword', 'name', 'text', 'target', 'longitud']
world_train_2 = pd.merge(world, train, how='inner', on='name')
world_train_2.count()

train.columns = ['id', 'keyword', 'iso_a3', 'text', 'target', 'longitud']
world_train_3 = pd.merge(world, train, how='inner', on='iso_a3')
world_train_3.count()

frames = [world_train_1, world_train_2, world_train_3]
world_train_final = pd.concat(frames)
world_train_final.count()

world_train_final.head()

grafico = world_train_final.groupby('name').agg({'text':'count'})
grafico_new = grafico.copy()
grafico_new['nombre'] = grafico.index
grafico_new.columns = ['cantidad tw', 'nombre']
grafico_new.head()

world_new = world_train_final.copy()
world_new.columns = ['pop_est','continent', 'nombre', 'iso_a3', 'gdp_md_est', 'geometry','id', 'keyword', 'text', 'target', 'longitud']
grafico_final = pd.merge(world_new, grafico_new, on='nombre', how='left')
grafico_final.head()

fig, ax = plt.subplots(figsize=(15, 7))
ax.set_title('Cantidad de tweets por localidad', pad = 20, fontdict={'fontsize':20, 'color': '#4873ab'})
ax.set_xlabel('Longitud')
ax.set_ylabel('Latitud')
grafico_final.plot(column='cantidad tw', legend=True, ax=ax)

grafico_promedio_longitudes = grafico_final.groupby('nombre').agg({'longitud':'mean'})
grafico_promedio_longitudes_new = grafico_promedio_longitudes.copy()
grafico_promedio_longitudes_new['name'] = grafico_promedio_longitudes.index
grafico_promedio_longitudes_new.columns = ['promedio longitud', 'name']
grafico_promedio_longitudes_new.head()

grafico_2_final = pd.merge(world_train_final, grafico_promedio_longitudes_new, on='name', how='left')
grafico_2_final.head()

fig, ax = plt.subplots(figsize=(15, 7))
ax.set_title('Promedio de longitud de tweets por localidad', pad = 20, fontdict={'fontsize':20, 'color': '#69c62f'})
ax.set_xlabel('Longitud')
ax.set_ylabel('Latitud')
grafico_2_final.plot(column='promedio longitud', legend=True, ax=ax)

grafico_final['target'].sum()

def promedio(x):
  return (x.sum()/x.count())*100

mapa_new = grafico_final.groupby('nombre').agg({'target':promedio})
mapa_porcentaje_veracidad = mapa_new.copy()
mapa_porcentaje_veracidad['name'] = mapa_new.index
mapa_porcentaje_veracidad.columns = ['porcentaje veracidad','name']
mapa_porcentaje_veracidad.head(30)

grafico_3_final = pd.merge(world_train_final, mapa_porcentaje_veracidad, on='name', how='left')
grafico_3_final.head()

fig, ax = plt.subplots(figsize=(15, 7))
ax.set_title('Porcentaje de veracidad de los tweets por localidad', pad = 20, fontdict={'fontsize':20, 'color': '#4873ab'})
ax.set_xlabel('Longitud')
ax.set_ylabel('Latitud')
grafico_3_final.plot(column='porcentaje veracidad', legend=True, ax=ax)

def promedio(x):
  return (x.sum()/grafico_final['target'].sum())*100

mapa_new_2 = grafico_final.groupby('nombre').agg({'target':promedio})
mapa_porcentaje_veracidad_2 = mapa_new_2.copy()
mapa_porcentaje_veracidad_2['name'] = mapa_new_2.index
mapa_porcentaje_veracidad_2.columns = ['porcentaje veracidad','name']
mapa_porcentaje_veracidad_2.head(30)

grafico_4_final = pd.merge(world_train_final, mapa_porcentaje_veracidad_2, on='name', how='left')
grafico_4_final.head()

fig, ax = plt.subplots(figsize=(15, 7))
ax.set_title('Porcentaje de veracidad por localidad con respecto al total de tweets verdaderos', pad = 20, fontdict={'fontsize':16, 'color': '#4873ab'})
ax.set_xlabel('Longitud')
ax.set_ylabel('Latitud')
grafico_4_final.plot(column='porcentaje veracidad', legend=True, ax=ax)

tabla = grafico_final[['keyword', 'continent', 'target']].pivot_table(index='keyword',columns='continent',values='target', aggfunc='count')
tabla.index = tabla.index.str.replace('%20', ' ')
tabla

fig, ax = plt.subplots(figsize = (10, 14))
g = sns.heatmap(tabla, cmap='magma_r', linewidths=0.5, ax=ax)
g.set_title('Cantidad de tweets en cada continente por problemática (keyword)');
g.set_ylabel('Keyword');
g.set_xlabel('Continente');
plt.savefig('desastres_naturales_por_continente.png')

cities = geopandas.read_file(geopandas.datasets.get_path('naturalearth_cities'))
cities.count()

train.columns = ['id', 'keyword', 'name', 'text', 'target', 'longitud']
cities_train = pd.merge(cities, train, how='inner', on='name')
cities_train.head()

cities_train.count()

fig, ax = plt.subplots(figsize = (15, 15))
world.plot(ax=ax, color='white', edgecolor='black')
ax.set_title('Relación keyword - ciudad')

viridis = cm.get_cmap('plasma', 256)
newcolors = viridis(np.linspace(0, 1, 256))
cmap = ListedColormap(newcolors)

cities_train.plot(column='keyword', ax=ax, marker='o', markersize=30, legend=True, cmap=cmap)
ax.get_legend().set_bbox_to_anchor((1,1.3))
plt.show()

